{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion item classification - The Dataset\n",
    "<img src=\"https://hips.hearstapps.com/hmg-prod.s3.amazonaws.com/images/fashionquote13-1624393596.jpg\" alt=\"Getty.\" align=\"left\" width=\"300\" style=\"vertical-align:right;margin:0px 30px\"/>\n",
    "\n",
    "## How did we tackle this problem?\n",
    "We started by looking for an appropriate dataset on which to train our classifier:\n",
    "\n",
    "## Existing datasets\n",
    "\n",
    "- [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist): the grand-daddy of fashion datasets. Academic, unrealistic and contains only 10 classes.\n",
    "- [Fashion Gen](https://paperswithcode.com/dataset/fashion-gen): unrealistic for user taken pictures.\n",
    "- [Fashionpedia](https://fashionpedia.github.io/home/): More about modeling than the clothes themselves.\n",
    "- [Deep Fashion](https://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html): More realistic. 50 classes. More annotations than needed so could be helpful for further work. Still not quite representative of the pictures people would take.\n",
    "- [Alexey Grigorev's clothing dataset](https://medium.com/data-science-insider/clothing-dataset-5b72cd7c3f1f): Very representative pictures. Small dataset. Only 20 classes. Good for prototyping.\n",
    "\n",
    "## But could we do better?\n",
    "**Always.** As mentioned before, the closest existing product, to our minds, is Vinted.com:\n",
    "\n",
    "- Vast amounts of pictures. A lot more than `100 000 unique pictures` on US site alone.\n",
    "- `Labels`: \n",
    "\t- all items are categorized in an arborescence. Yeah! Quality labeling.\n",
    "\t- a search (`keyword`) will almost always return results. Powerful. But result quality needs to be assessed.\n",
    "\t\t- \tWe could end up creating a classifier than behaves like Vinted's search engine instead of doing better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We crawled their category arborescence (which they call the *catalog*) and discovered `575` categories (including parents). After removing the irrelevant ones, we were left with `491` categories: a vast improvement on our most promising existing dataset from Grigorev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "! pip install -q fake_useragent numpy pandas requests pillow tqdm kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:59:58.672053Z",
     "iopub.status.busy": "2022-04-22T13:59:58.671760Z",
     "iopub.status.idle": "2022-04-22T13:59:58.831811Z",
     "shell.execute_reply": "2022-04-22T13:59:58.831159Z",
     "shell.execute_reply.started": "2022-04-22T13:59:58.672016Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "import json\n",
    "import io\n",
    "import datetime\n",
    "import zipfile\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import requests\n",
    "import fake_useragent\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from urllib.error import HTTPError\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import thread_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize fake_useragent's DB. Can comment afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows; U; Windows NT 6.1; tr-TR) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_useragent.UserAgent().random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few helper functions:\n",
    "\n",
    "* ``get_session()``: Initiates a session with the API and gets the necessary cookies.\n",
    "* ``get_url()``: A fault tolerant ``requests.get()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-22T13:59:58.833971Z",
     "iopub.status.busy": "2022-04-22T13:59:58.832990Z",
     "iopub.status.idle": "2022-04-22T13:59:58.841487Z",
     "shell.execute_reply": "2022-04-22T13:59:58.840009Z",
     "shell.execute_reply.started": "2022-04-22T13:59:58.833933Z"
    }
   },
   "outputs": [],
   "source": [
    "domain = \"com\"\n",
    "VINTED_URL = f\"https://www.vinted.{domain}\"\n",
    "VINTED_AUTH_URL = f\"https://www.vinted.{domain}/auth/token_refresh\"\n",
    "VINTED_API_URL = f\"https://www.vinted.{domain}/api/v2/catalog/items\"\n",
    "\n",
    "def beautiful_sleep(seconds, message):\n",
    "    \"Nicely print a message and sleep for a given number of seconds.\"\n",
    "    for i in reversed(range(seconds)):\n",
    "        print(f\"{message}. Sleeping {i} seconds.    \\r\", end=\"\")\n",
    "        time.sleep(1)\n",
    "\n",
    "def get_session(sleep_counter=2):\n",
    "    \"Get a session (cookies) with a fake user agent. Sleep if needed then calls itself.\"\n",
    "    session = requests.Session()\n",
    "    user_agent = fake_useragent.UserAgent().random # Maybe helpful?\n",
    "    session.headers.update({\"User-Agent\": user_agent})\n",
    "\n",
    "    response = session.post(VINTED_AUTH_URL, headers={\"User-Agent\": user_agent}) # Set cookies\n",
    "\n",
    "    if response.status_code == 429:\n",
    "        bench_time = int(response.headers[\"Retry-After\"]) + 10 # at this point...\n",
    "        beautiful_sleep(bench_time, f\"[get_session {response.status_code}]\")\n",
    "        return get_session()\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        time.sleep(sleep_counter)\n",
    "        return get_session(sleep_counter * 2)\n",
    "\n",
    "    return session\n",
    "\n",
    "def get_url(url, session=None, params=None, sleep_counter=2):\n",
    "    \"Get a response from a given URL. Sleep if needed then calls itself.\"\n",
    "    if session is None:\n",
    "        session = get_session()\n",
    "    if params is None:\n",
    "        params = dict()\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=5)\n",
    "    except TimeoutError:\n",
    "        print(f\"Timeout. Skipping\")\n",
    "        return response, None\n",
    "        \n",
    "    if response.status_code == 500:\n",
    "        return response, None\n",
    "    if response.status_code == 429:\n",
    "        bench_time = int(response.headers[\"Retry-After\"]) + 10 # at this point...\n",
    "        beautiful_sleep(bench_time, f\"[get_url {response.status_code}]\")\n",
    "        return get_url(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        time.sleep(sleep_counter)\n",
    "        return get_url(url, params=params, sleep_counter=sleep_counter * 2)\n",
    "        \n",
    "    return response, session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by getting the catalog. This is a tree of all the item categories sold on Vinted.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catalog():\n",
    "    \"Get the catalog (category tree) from Vinted.\"\n",
    "    url = f\"https://www.vinted.com/vetements?\"\n",
    "    res, session = get_url(url)\n",
    "    matches = re.findall('({.+})', res.text)\n",
    "    sub_matches = list(m for m in matches if 'code' in m) # should be in the regex. Oh well.\n",
    "\n",
    "    catalog = json.loads(sub_matches[0]).get('catalogTree')\n",
    "    return catalog  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1904, 'title': 'Women', 'code': 'WOMEN_ROOT', 'material_group_id': None, 'material_group_ids': [], 'size_group_id': 4, 'size_group_ids': [4, 7, 53, 52, 30], 'shippable': True, 'author_field_visibility': 0, 'brand_field_visibility': 1, 'book_title_field_visibility': 0, 'color_field_visibility': 1, 'isbn_field_visibility': 0, 'size_field_visibility': 1, 'material_field_visibility': 0, 'location_field_visible': False, 'condition_field_visible': True, 'restricted_to_status_id': None, 'landing': None, 'allow_browsing_subcategories': True, 'package_size_ids': [1, 2, 3, 4], 'order': 0, 'item_count': 231519112, 'photo': {'url': 'https://images1.vinted.net/t/03_00ffd_tLaPvmCTXrgwxpvwY4mqZffA/1651062589.png?s=d74cf4c3d78c2156ebf59cda6b1a429af65b4a29', 'thumbnails': [{'type': 'thumb24', 'height': 24, 'width': 24, 'url': 'https://images1.vinted.net/t/03_00ffd_tLaPvmCTXrgwxpvwY4mqZffA/24x24/1651062589.png?s=3b0064bdf0a4f2d993b2dd8d9ef60b3b4d443023'}, {'type': 'thumb48', 'height': 48, 'width'\n"
     ]
    }
   ],
   "source": [
    "catalog = get_catalog() # this is a tree\n",
    "while catalog is None:\n",
    "    catalog = get_catalog()\n",
    "\n",
    "print(str(catalog[0])[:1000]) # truncated for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the categories and their parents as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {\n",
    "        'Home', \n",
    "        'Beauty', \n",
    "        'Grooming', \n",
    "        'Toys & games',\n",
    "        'Baby care',\n",
    "        'Strollers',\n",
    "        'Ride-on toys',\n",
    "        'Chairs',\n",
    "        \"Kids' furniture\",\n",
    "        'School supplies',\n",
    "        \"Other kids' items\"\n",
    "    }\n",
    "\n",
    "def get_categories(catalog, stop_words, parents=None):\n",
    "    \"Get all categories from a catalog. Recursive.\"\n",
    "    if parents is None:\n",
    "        parents = []\n",
    "    for c in catalog:\n",
    "        if c.get('title') in stop_words:\n",
    "            continue\n",
    "        if c.get('catalogs'):\n",
    "            yield from get_categories(c.get('catalogs'), stop_words, parents + [(c.get('id'), c.get('title'))])\n",
    "        else:\n",
    "            yield parents + [(c.get('id'), c.get('title'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "[[(1904, 'Women'), (4, 'Clothes'), (1037, 'Coats & jackets'), (1907, 'Coats'), (1087, 'Parkas')]]\n"
     ]
    }
   ],
   "source": [
    "list_categories = list(get_categories(catalog, stop_words=stop_words))\n",
    "print(len(list_categories))\n",
    "print(list_categories[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_categories(list_categories):\n",
    "    for categories in list_categories:\n",
    "        yield categories[-1][0], ('__'.join(category[-1] for category in categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "[(1087, 'Women__Clothes__Coats & jackets__Coats__Parkas')]\n"
     ]
    }
   ],
   "source": [
    "list_categories = list(flatten_categories(list_categories))\n",
    "print(len(list_categories))\n",
    "print(list_categories[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_labels(categories):\n",
    "    for id, label in categories:\n",
    "        yield id, (label.lower()\n",
    "                    .replace(\" \", \"_\")\n",
    "                    .replace(\"-\", \"_\")\n",
    "                    .replace(\"'\", \"\")\n",
    "                    .replace(\"&\", \"and\")\n",
    "                    .replace('¾', 'three_quarters')\n",
    "                    .replace(',', '')\n",
    "                    .replace('/', '_')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "[(1087, 'women__clothes__coats_and_jackets__coats__parkas')]\n"
     ]
    }
   ],
   "source": [
    "list_categories = list(sanitize_labels(list_categories))\n",
    "print(len(list_categories))\n",
    "print(list_categories[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we still needed to assess whether these categories yielded enough results to be even attemptable. We sampled a small number of categories manually, particularly the niche ones and decided the dataset would be good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We were in business!\n",
    "How could we get the pictures and their labels easily? We found a sparsely documented library, [pyVinted](https://github.com/aime-risson/vinted-api-wrapper), which makes use of an undocumented Vinted api, and took the important parts and build an api friendly and fault-tolerant crawler to:\n",
    "\n",
    "1. (Try to) get 300 urls for each category. 300 is a soft limit for the maximum number of items per page.\n",
    "2. Download the pictures.\n",
    "3. Save them to disk into train and validation folders containing a flat hierarchy of folders, one for each class, with the matching pictures. Each picture is labeled by the item id given by Vinted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all picture urls... Time to go for a coffee.\n",
    "\n",
    "``get_dataframe()`` has basic resuming capabilities. It will try to get 300 pictures of each terminal category but will also attribute them to their parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picture_urls_by_category_id(category):\n",
    "    \"Get the picture urls for a given category.\"\n",
    "    category_id, label = category\n",
    "    session = None # will be created in get_url\n",
    "    params = {\"catalog_ids\": category_id, \"per_page\": 300, 'page':1} # Seems like max per_page around 300+\n",
    "\n",
    "    response, session = get_url(VINTED_API_URL, session, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return # skip this category\n",
    "    \n",
    "    items = response.json()[\"items\"]\n",
    "\n",
    "    res = list()\n",
    "    for item in items:\n",
    "        try: \n",
    "            res.append((item[\"id\"], item[\"photo\"][\"url\"], label))\n",
    "        except TypeError:\n",
    "            continue\n",
    "    return res\n",
    "    \n",
    "def get_picture_urls(categories, num_workers=1):\n",
    "    \"Get a dataframe with all picture urls.\"\n",
    "    return thread_map(get_picture_urls_by_category_id, categories, max_workers=num_workers) \n",
    "\n",
    "def make_df(gen):\n",
    "    return pd.DataFrame(itertools.chain(*gen), columns=['id', 'url', 'label']).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.76s/it]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 3 # number of categories to sample. To get vinted_dev\n",
    "df = make_df(get_picture_urls(list_categories[:sample_size], num_workers=1)) # For now, just one worker. Need to figure out how not to get banned.\n",
    "#df = make_df(get_picture_urls(list_categories, num_workers=1)) # To get all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"vinted_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"vinted.csv\").set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take care of duplicates. Unisex items are both in women and men categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    duplicates = df.index.duplicated()\n",
    "    for row in df[duplicates].iterrows(): # somebody finds something faster please\n",
    "        df.loc[row[0], 'label'] = row[1]['label'].replace(\"women\", \"unisex\").replace(\"men\", \"unisex\")\n",
    "    return df[~df.index.duplicated(keep='first')] # drop the duplicates    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has duplicates? False\n",
      "Has duplicates? False\n",
      "Unique labels: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Has duplicates? {df.index.has_duplicates}\")\n",
    "df = remove_duplicates(df)\n",
    "print(f\"Has duplicates? {df.index.has_duplicates}\")\n",
    "print(f\"Unique labels: {len(df.label.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training purposes, it is easier to make sure all the classes are represented in both the training and the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_singleton_categories(df):\n",
    "    counts_labels = df.groupby(\"label\")[\"label\"].transform(len) >= 2 # at least 2 pictures\n",
    "    return df[counts_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 899 pictures\n",
      "After: 899 pictures\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {len(df.index)} pictures\")\n",
    "df = remove_singleton_categories(df)\n",
    "print(f\"After: {len(df.index)} pictures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the pictures, resize them and save them to disk in all the appropriate folders. Time for another coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params(df, target_folder):\n",
    "    # Prapare the params for the map function\n",
    "    for id, row in df.iterrows():\n",
    "        yield (id, row.url, row.label, target_folder)\n",
    "\n",
    "def download_picture(args):\n",
    "    id, url, label, target_folder = args\n",
    "    filename = os.path.join(target_folder, label, f\"{id}.jpeg\")\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        req = requests.get(url)\n",
    "        if req.status_code != 200:\n",
    "            return # skip\n",
    "        # resize picture\n",
    "        ratio = 800/224 # from 800 px to 224 px for the largest side\n",
    "        picture = Image.open(io.BytesIO(req.content))\n",
    "        picture = picture.resize((int(picture.width/ratio), int(picture.height/ratio)))\n",
    "        # save picture\n",
    "        picture.save(filename, quality=50)\n",
    "\n",
    "def make_dirs(df, target_folder):\n",
    "    for label in df.label.unique():\n",
    "        os.makedirs(os.path.join(target_folder, label), exist_ok=True)\n",
    "\n",
    "def download_pictures(df, target_folder):\n",
    "    \"Download all pictures from a given list of urls.\"\n",
    "    thread_map(download_picture, list(params(df, target_folder)), tqdm_class=tqdm, max_workers=128) # list() is only needed to make a pretty progress bar at the expense of memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899/899 [00:03<00:00, 243.26it/s]\n"
     ]
    }
   ],
   "source": [
    "target_folder = \"vinted_dev\"\n",
    "make_dirs(df, target_folder)\n",
    "download_pictures(df, target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Archive both the pictures for upload to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_val(folder):\n",
    "    train_folder, val_folder = f\"{folder}_train\", f\"{folder}_val\"\n",
    "    # for every sub folder in folder\n",
    "    for label in tqdm(os.listdir(folder), desc=\"Making train and val sets\"):\n",
    "        # if label is not a folder\n",
    "        if not os.path.isdir(os.path.join(folder, label)):\n",
    "            continue\n",
    "        # make train and val sub folders\n",
    "        os.makedirs(os.path.join(train_folder, label), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_folder, label), exist_ok=True)\n",
    "        # for every file in sub folder\n",
    "        filenames = os.listdir(os.path.join(folder, label))\n",
    "        random.shuffle(filenames)\n",
    "        modulo = min(len(filenames), 5) # val is 1/5th but should contain at least 1 element\n",
    "        for i in range(len(filenames)):\n",
    "            if i % modulo == 0:\n",
    "                shutil.copy(os.path.join(folder, label, filenames[i]), os.path.join(val_folder, label, filenames[i]))\n",
    "            else:\n",
    "                shutil.copy(os.path.join(folder, label, filenames[i]), os.path.join(train_folder, label, filenames[i]))\n",
    "\n",
    "def zip_train_val(prefix):\n",
    "    train_folder, val_folder = f\"{prefix}_train\", f\"{prefix}_val\"\n",
    "    # zip train_folder and val_foder in to a single zip file\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    with zipfile.ZipFile(f\"{prefix}_{today}.zip\", \"w\") as zip_file:\n",
    "        for label in tqdm(os.listdir(train_folder), desc=\"Zipping the train set\"):\n",
    "            for file in os.listdir(os.path.join(train_folder, label)):\n",
    "                zip_file.write(os.path.join(train_folder, label, file))\n",
    "        for label in tqdm(os.listdir(val_folder), desc=\"Zipping the val set\"):\n",
    "            for file in os.listdir(os.path.join(val_folder, label)):\n",
    "                zip_file.write(os.path.join(val_folder, label, file))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making train and val sets: 100%|██████████| 3/3 [00:00<00:00, 16.54it/s]\n",
      "Zipping the train set: 100%|██████████| 3/3 [00:00<00:00, 97.99it/s]\n",
      "Zipping the val set: 100%|██████████| 3/3 [00:00<00:00, 370.65it/s]\n"
     ]
    }
   ],
   "source": [
    "make_train_val(folder=target_folder)\n",
    "zip_train_val(prefix=target_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Éditer les Méta-Données",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
